{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa4ce13-8e39-432c-8948-6e8f9fa92575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "# import pacmap\n",
    "\n",
    "# set the device we're using for training.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "# print(torch.cuda.is_available())\n",
    "# torch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec53c696-d68e-4939-9743-75bc038b6d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.4\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdong/.conda/envs/myenv/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d978f13-9507-487f-9f94-6f694f3a248b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the directory containing the FITS files\n",
    "data_dir = '/pool001/rarcodia/eROSITA_public/data/eRASS1_lc_rebinned'\n",
    "\n",
    "def read_inaccessible_lightcurves():\n",
    "    \"\"\"\n",
    "    Read the list of inaccessible light curves from the text file in the notebook directory.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of file paths that were inaccessible\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(os.getcwd(), \"inaccessible_lightcurves.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            # Read all lines and remove any trailing whitespace\n",
    "            inaccessible_files = [line.strip() for line in f.readlines()]\n",
    "        return inaccessible_files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No inaccessible light curves file found at {file_path}\")\n",
    "        return []\n",
    "\n",
    "# Function to load a single FITS file and return as a Pandas DataFrame\n",
    "def load_light_curve(file_path, band=1):\n",
    "    \"\"\"\n",
    "    Load light curve data from a FITS file and return a Pandas DataFrame including asymmetric errors (ERRM and ERRP).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the FITS file.\n",
    "        band (int): Energy band index to load data for (default: 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame with light curve data, or None if file is skipped.\n",
    "    \"\"\"\n",
    "    with fits.open(file_path) as hdul:\n",
    "        data = hdul[1].data  # Assuming light curve data is in the second HDU\n",
    "        try:\n",
    "            light_curve = pd.DataFrame({\n",
    "                'TIME': data['TIME'],\n",
    "                'TIMEDEL': data['TIMEDEL'],\n",
    "                'RATE': data['RATE'][:, band],  # Light curve intensity\n",
    "                'ERRM': data['RATE_ERRM'][:, band],  # Negative error\n",
    "                'ERRP': data['RATE_ERRP'][:, band],  # Positive error\n",
    "                'SYM_ERR': (data['RATE_ERRM'][:, band] + data['RATE_ERRP'][:, band]) / 2,  # Symmetric error approximation\n",
    "            })\n",
    "            # Attach metadata as attributes\n",
    "            light_curve.attrs['FILE_NAME'] = os.path.basename(file_path)\n",
    "            light_curve.attrs['OUTLIER'] = False\n",
    "            return light_curve\n",
    "        except KeyError:\n",
    "            print(f\"Skipping file {file_path}: some key not found\")\n",
    "            return None\n",
    "\n",
    "def load_all_fits_files(data_dir = '/pool001/rarcodia/eROSITA_public/data/eRASS1_lc_rebinned'):\n",
    "    \"\"\"\n",
    "    Loads all fits files\n",
    "    \n",
    "    Parameters:\n",
    "        data_dir (str): The filepath where the data is located\n",
    "        \n",
    "    Returns:\n",
    "        fits_files (list): A list of all the fits files\n",
    "    \"\"\"\n",
    "    \n",
    "    return glob.glob(os.path.join(data_dir, \"*.fits\"))\n",
    "\n",
    "def load_n_light_curves(n, fits_files, band = 'all'):\n",
    "    \"\"\"\n",
    "    Loads a specified amount of light curves to analyze.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): Number of light curves to load.\n",
    "        fits_files (list): A list of all the fits files\n",
    "        \n",
    "    Returns:\n",
    "        light_curves_1 (list): A list of n light curves in 0.2-0.6 keV,\n",
    "        light_curves_2 (list): A list of n light curves in 0.6-2.3keV\n",
    "        light_curves_3 (list): A list of n light curves in 2.3-5.0keV\n",
    "    \"\"\"\n",
    "    \n",
    "    inaccess_files = read_inaccessible_lightcurves()\n",
    "\n",
    "    # Randomly select n files\n",
    "    fits_files = random.sample(fits_files, n)\n",
    "    \n",
    "    temp = []\n",
    "    for lc in  fits_files:\n",
    "        if lc not in inaccess_files:\n",
    "            temp.append(lc)\n",
    "    fits_files = temp\n",
    "    \n",
    "    # Load all bands of the light curves into a list of DataFrames\n",
    "    if band == 'all':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "    \n",
    "        return light_curves_1, light_curves_2, light_curves_3\n",
    "    elif band == 'low':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        return light_curves_1\n",
    "    elif band == 'med':\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        return light_curves_2\n",
    "    elif band == 'high':\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "        return light_curves_3\n",
    "    else:\n",
    "        raise KeyError(\"Input for Band is not valid\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e57fd-c5f2-4a53-ac91-153bf7b549d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started loading lcs\n"
     ]
    }
   ],
   "source": [
    "fits_files = load_all_fits_files()\n",
    "print('started loading lcs')\n",
    "light_curves_sample = load_n_light_curves(400, fits_files, band = \"med\")\n",
    "print('finished loading lcs')\n",
    "print(len(light_curves_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b504c23-3375-4bfd-a976-985102e661ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into train set and test set\n",
    "def partition_data(light_curves, test_size=0.2, val_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Partition a list of light curves into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        light_curves (list): List of light curve DataFrames.\n",
    "        test_size (float): Proportion of data to use for the test set.\n",
    "        val_size (float): Proportion of train data to use for the validation set.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_set (list): List of light curves for training.\n",
    "        val_set (list): List of light curves for validation (if val_size > 0).\n",
    "        test_set (list): List of light curves for testing.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # Split into train+val and test sets\n",
    "    train_val_set, test_set = train_test_split(light_curves, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    if val_size > 0:\n",
    "        # Split train_val into train and validation sets\n",
    "        train_size = 1 - val_size\n",
    "        train_set, val_set = train_test_split(train_val_set, test_size=val_size, random_state=random_seed)\n",
    "        return train_set, val_set, test_set\n",
    "    else:\n",
    "        # If no validation set is needed, return only train and test sets\n",
    "        return train_val_set, test_set\n",
    "train_dataset, val_dataset, test_dataset = partition_data(light_curves_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84232f-a488-4302-9e40-329495c3bd24",
   "metadata": {},
   "source": [
    "## Code for training autoencoder ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321a066-bf36-4524-846e-d22be97a3d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the evidence lower bound loss for training autoencoders\n",
    "def ELBO(x_hat, x, mu, logvar):\n",
    "    # the reconstruction loss\n",
    "    MSE = torch.nn.MSELoss(reduction='sum')(x_hat, x)\n",
    "\n",
    "    # the KL-divergence between the latent distribution and a multivariate normal\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n",
    "# our encoder class\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=400, num_layers=4, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: tensor of shape (batch_size, seq_length, input_size)\n",
    "        # lengths: tensor of shape (batch_size), containing the lengths of each sequence in the batch\n",
    "\n",
    "        # NOTE: Here we use the pytorch functions pack_padded_sequence and pad_packed_sequence, which\n",
    "        # allow us to\n",
    "        packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed_x)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output, hidden\n",
    "\n",
    "# our decoder class\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=7, hidden_size=400, output_size=4, num_layers=4, dropout=0.2\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, lengths=None):\n",
    "        if lengths is not None:\n",
    "            # unpad the light curves so that our latent representations learn only from real data\n",
    "            packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            packed_output, hidden = self.gru(packed_x, hidden)\n",
    "\n",
    "            # re-pad the light curves so that they can be processed elsewhere\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        else:\n",
    "            output, hidden = self.gru(x, hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "class RNN_VAE(torch.nn.Module):\n",
    "    \"\"\"RNN-VAE: A Variational Auto-Encoder with a Recurrent Neural Network Layer as the Encoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size=3, hidden_size=400, latent_size=50, dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_size: int, batch_size x sequence_length x input_dim\n",
    "        hidden_size: int, output size\n",
    "        latent_size: int, latent z-layer size\n",
    "        num_gru_layer: int, number of layers\n",
    "        \"\"\"\n",
    "        super(RNN_VAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layers = 4\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.enc = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=self.num_layers, dropout=self.dropout)\n",
    "\n",
    "        self.dec = Decoder(\n",
    "            input_size=latent_size,\n",
    "            output_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=self.dropout,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc21 = torch.nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc22 = torch.nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc3 = torch.nn.Linear(self.latent_size, self.hidden_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn(mu.shape).to(device)*torch.exp(0.5*logvar)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # encode input space\n",
    "        enc_output, enc_hidden = self.enc(x, lengths)\n",
    "\n",
    "        # Correctly accessing the hidden state of the last layer\n",
    "        enc_h = enc_hidden[-1].to(device)  # This is now [batch_size, hidden_size]\n",
    "\n",
    "        # extract latent variable z\n",
    "        mu = self.fc21(enc_h)\n",
    "        logvar = self.fc22(enc_h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_ = self.fc3(z)\n",
    "        h_ = h_.unsqueeze(0)  # Add an extra dimension for num_layers\n",
    "        # Repeat the hidden state for each layer\n",
    "        h_ = h_.repeat(self.dec.num_layers, 1, 1)  # Now h_ is [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # decode latent space\n",
    "        z = z.repeat(1, seq_len, 1)\n",
    "        z = z.view(batch_size, seq_len, self.latent_size).to(device)\n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden = h_.contiguous()\n",
    "        x_hat, hidden = self.dec(z, hidden)\n",
    "\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344528a-96ff-41d0-ae6c-74e044052394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataLoader\n",
    "def collate_fn_err(batch):\n",
    "    rate = [torch.tensor(lc['RATE'].values, dtype=torch.float32) for lc in batch]\n",
    "    lowErr = [torch.tensor(lc['ERRM'].values, dtype=torch.float32) for lc in batch]\n",
    "    upErr = [torch.tensor(lc['ERRP'].values, dtype=torch.float32) for lc in batch]\n",
    "    \n",
    "    sequences = [torch.stack([r, le, ue], dim=-1) for r, le, ue in zip(rate, lowErr, upErr)]\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    # Pad sequences\n",
    "    x = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return x, lengths\n",
    "    \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn_err, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d31e88-de7a-42e8-b677-cf473ceaeefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = RNN_VAE(input_size=3, hidden_size=400, latent_size=50).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Number of epochs\n",
    "nepochs = 10\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are PyTorch DataLoader objects\n",
    "# The data loaders should yield batches with:\n",
    "# - x: tensor of shape (batch_size, seq_length, 1)\n",
    "# - lengths: tensor of sequence lengths\n",
    "print(\"Beginning training...\")\n",
    "for epoch in range(nepochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x1, x2, x3, lengths = batch\n",
    "        x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat, mu, logvar = model(x1, x2, x3, lengths)\n",
    "\n",
    "        # Compute loss (use x1 as target or modify if necessary)\n",
    "        loss = ELBO(x_hat.squeeze(-1), x[..., 0], mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        print(1)\n",
    "\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, lengths = batch\n",
    "            x = x.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            x_hat, mu, logvar = model(x, lengths)\n",
    "\n",
    "            # Compute validation loss (focus on RATE)\n",
    "            valid_loss += ELBO(x_hat.squeeze(-1), x[..., 0], mu, logvar).item()  # Compare only with RATE\n",
    "\n",
    "    # Average losses\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.2f} | Valid Loss: {valid_loss:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
