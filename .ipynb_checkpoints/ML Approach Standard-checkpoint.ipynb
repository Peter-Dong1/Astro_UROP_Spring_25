{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e942adad-4b34-402d-a9de-0de1c0ee7921",
   "metadata": {},
   "source": [
    "# Machine Learning Approach #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce98a1b5-2714-4c7b-8d01-3977c07bf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "# import pacmap\n",
    "\n",
    "# set the device we're using for training.\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "# print(torch.cuda.is_available())\n",
    "# torch.backends.cudnn.enabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a273c9-7a73-4764-9ad2-89a3cbd9d068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "12.4\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdong/.conda/envs/myenv/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647429097/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880f3e6c-c82b-42f3-ba12-5a19b57426b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the evidence lower bound loss for training autoencoders\n",
    "def ELBO(x_hat, x, mu, logvar):\n",
    "    # the reconstruction loss\n",
    "    MSE = torch.nn.MSELoss(reduction='sum')(x_hat, x)\n",
    "\n",
    "    # the KL-divergence between the latent distribution and a multivariate normal\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc91f69-2d79-47c6-a356-23be5f595af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our encoder class\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size=7, hidden_size=400, num_layers=4, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: tensor of shape (batch_size, seq_length, input_size)\n",
    "        # lengths: tensor of shape (batch_size), containing the lengths of each sequence in the batch\n",
    "\n",
    "        # NOTE: Here we use the pytorch functions pack_padded_sequence and pad_packed_sequence, which\n",
    "        # allow us to\n",
    "        packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, hidden = self.gru(packed_x)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return output, hidden\n",
    "\n",
    "# our decoder class\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=7, hidden_size=400, output_size=4, num_layers=4, dropout=0.2\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, lengths=None):\n",
    "        if lengths is not None:\n",
    "            # unpad the light curves so that our latent representations learn only from real data\n",
    "            packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            packed_output, hidden = self.gru(packed_x, hidden)\n",
    "\n",
    "            # re-pad the light curves so that they can be processed elsewhere\n",
    "            output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        else:\n",
    "            output, hidden = self.gru(x, hidden)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "class RNN_VAE(torch.nn.Module):\n",
    "    \"\"\"RNN-VAE: A Variational Auto-Encoder with a Recurrent Neural Network Layer as the Encoder.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_size=7, hidden_size=400, latent_size=50, dropout=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_size: int, batch_size x sequence_length x input_dim\n",
    "        hidden_size: int, output size\n",
    "        latent_size: int, latent z-layer size\n",
    "        num_gru_layer: int, number of layers\n",
    "        \"\"\"\n",
    "        super(RNN_VAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # dimensions\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layers = 4\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.enc = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=self.num_layers, dropout=self.dropout)\n",
    "\n",
    "        self.dec = Decoder(\n",
    "            input_size=latent_size,\n",
    "            output_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout=self.dropout,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc21 = torch.nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc22 = torch.nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc3 = torch.nn.Linear(self.latent_size, self.hidden_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn(mu.shape).to(device)*torch.exp(0.5*logvar)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # encode input space\n",
    "        enc_output, enc_hidden = self.enc(x, lengths)\n",
    "\n",
    "        # Correctly accessing the hidden state of the last layer\n",
    "        enc_h = enc_hidden[-1].to(device)  # This is now [batch_size, hidden_size]\n",
    "\n",
    "        # extract latent variable z\n",
    "        mu = self.fc21(enc_h)\n",
    "        logvar = self.fc22(enc_h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_ = self.fc3(z)\n",
    "        h_ = h_.unsqueeze(0)  # Add an extra dimension for num_layers\n",
    "        # Repeat the hidden state for each layer\n",
    "        h_ = h_.repeat(self.dec.num_layers, 1, 1)  # Now h_ is [num_layers, batch_size, hidden_size]\n",
    "\n",
    "        # decode latent space\n",
    "        z = z.repeat(1, seq_len, 1)\n",
    "        z = z.view(batch_size, seq_len, self.latent_size).to(device)\n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden = h_.contiguous()\n",
    "        x_hat, hidden = self.dec(z, hidden)\n",
    "\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b0e29-ec21-4939-892e-5cb4cef72783",
   "metadata": {},
   "source": [
    "## Preprocess Light Curves ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2456976c-8237-485e-997b-14b2fb0ed7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a 2d dataframe of light curves\n",
    "def create_dataframe_of_light_curves(n, fits_files, band = 'all'):\n",
    "    \"\"\"\n",
    "    Create a dataframe of dataframes where each row corresponds to a light curve\n",
    "    and each column represents a specific energy band.\n",
    "\n",
    "    Parameters:\n",
    "        fits_files (list): List of paths to FITS files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe of dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file_path in fits_files:\n",
    "        light_curve_low = load_light_curve(file_path, band=0)  # Low band\n",
    "        light_curve_med = load_light_curve(file_path, band=1)  # Medium band\n",
    "        light_curve_high = load_light_curve(file_path, band=2)  # High band\n",
    "\n",
    "        data.append({\n",
    "            'file_name': os.path.basename(file_path),\n",
    "            'low_band': light_curve_low,\n",
    "            'medium_band': light_curve_med,\n",
    "            'high_band': light_curve_high\n",
    "        })\n",
    "\n",
    "    # Convert the list of dictionaries into a dataframe\n",
    "    df_of_dfs = pd.DataFrame(data)\n",
    "    return df_of_dfs\n",
    "\n",
    "# Example usage:\n",
    "# fits_files = load_all_fits_files(data_dir)\n",
    "# df_of_dfs = create_dataframe_of_dataframes(fits_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace5c17-214a-4d3b-aca4-b36325388177",
   "metadata": {},
   "source": [
    "### Load Light Curves Simple ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a719c155-7573-4609-9584-5e0cc82177df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to the directory containing the FITS files\n",
    "data_dir = '/pool001/rarcodia/eROSITA_public/data/eRASS1_lc_rebinned'\n",
    "\n",
    "def read_inaccessible_lightcurves():\n",
    "    \"\"\"\n",
    "    Read the list of inaccessible light curves from the text file in the notebook directory.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of file paths that were inaccessible\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(os.getcwd(), \"inaccessible_lightcurves.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            # Read all lines and remove any trailing whitespace\n",
    "            inaccessible_files = [line.strip() for line in f.readlines()]\n",
    "        return inaccessible_files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No inaccessible light curves file found at {file_path}\")\n",
    "        return []\n",
    "\n",
    "# Function to load a single FITS file and return as a Pandas DataFrame\n",
    "def load_light_curve(file_path, band=1):\n",
    "    \"\"\"\n",
    "    Load light curve data from a FITS file and return a Pandas DataFrame including asymmetric errors (ERRM and ERRP).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the FITS file.\n",
    "        band (int): Energy band index to load data for (default: 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: DataFrame with light curve data, or None if file is skipped.\n",
    "    \"\"\"\n",
    "    with fits.open(file_path) as hdul:\n",
    "        data = hdul[1].data  # Assuming light curve data is in the second HDU\n",
    "        try:\n",
    "            light_curve = pd.DataFrame({\n",
    "                'TIME': data['TIME'],\n",
    "                'TIMEDEL': data['TIMEDEL'],\n",
    "                'RATE': data['RATE'][:, band],  # Light curve intensity\n",
    "                'ERRM': data['RATE_ERRM'][:, band],  # Negative error\n",
    "                'ERRP': data['RATE_ERRP'][:, band],  # Positive error\n",
    "                'SYM_ERR': (data['RATE_ERRM'][:, band] + data['RATE_ERRP'][:, band]) / 2,  # Symmetric error approximation\n",
    "            })\n",
    "            # Attach metadata as attributes\n",
    "            light_curve.attrs['FILE_NAME'] = os.path.basename(file_path)\n",
    "            light_curve.attrs['OUTLIER'] = False\n",
    "            return light_curve\n",
    "        except KeyError:\n",
    "            print(f\"Skipping file {file_path}: some key not found\")\n",
    "            return None\n",
    "\n",
    "def load_all_fits_files(data_dir = '/pool001/rarcodia/eROSITA_public/data/eRASS1_lc_rebinned'):\n",
    "    \"\"\"\n",
    "    Loads all fits files\n",
    "    \n",
    "    Parameters:\n",
    "        data_dir (str): The filepath where the data is located\n",
    "        \n",
    "    Returns:\n",
    "        fits_files (list): A list of all the fits files\n",
    "    \"\"\"\n",
    "    \n",
    "    return glob.glob(os.path.join(data_dir, \"*.fits\"))\n",
    "\n",
    "def load_n_light_curves(n, fits_files, band = 'all'):\n",
    "    \"\"\"\n",
    "    Loads a specified amount of light curves to analyze.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): Number of light curves to load.\n",
    "        fits_files (list): A list of all the fits files\n",
    "        \n",
    "    Returns:\n",
    "        light_curves_1 (list): A list of n light curves in 0.2-0.6 keV,\n",
    "        light_curves_2 (list): A list of n light curves in 0.6-2.3keV\n",
    "        light_curves_3 (list): A list of n light curves in 2.3-5.0keV\n",
    "    \"\"\"\n",
    "    \n",
    "    inaccess_files = read_inaccessible_lightcurves()\n",
    "\n",
    "    # Randomly select n files\n",
    "    fits_files = random.sample(fits_files, n)\n",
    "    \n",
    "    temp = []\n",
    "    for lc in  fits_files:\n",
    "        if lc not in inaccess_files:\n",
    "            temp.append(lc)\n",
    "    fits_files = temp\n",
    "    \n",
    "    # Load all bands of the light curves into a list of DataFrames\n",
    "    if band == 'all':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "    \n",
    "        return light_curves_1, light_curves_2, light_curves_3\n",
    "    elif band == 'low':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        return light_curves_1\n",
    "    elif band == 'med':\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        return light_curves_2\n",
    "    elif band == 'high':\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "        return light_curves_3\n",
    "    else:\n",
    "        raise KeyError(\"Input for Band is not valid\")\n",
    "        \n",
    "    \n",
    "\n",
    "def load_all_light_curves(fits_files):\n",
    "    \"\"\"\n",
    "    Loads a specified amount of light curves to analyze.\n",
    "    \n",
    "    Parameters:\n",
    "        n (int): Number of light curves to load.\n",
    "        fits_files (list): A list of all the fits files\n",
    "        \n",
    "    Returns:\n",
    "        light_curves_1 (list): A list of n light curves in 0.2-0.6 keV,\n",
    "        light_curves_2 (list): A list of n light curves in 0.6-2.3keV\n",
    "        light_curves_3 (list): A list of n light curves in 2.3-5.0keV\n",
    "    \"\"\"\n",
    "    \n",
    "    inaccess_files = read_inaccessible_lightcurves()\n",
    "    \n",
    "    temp = []\n",
    "    for lc in  fits_files:\n",
    "        if lc not in inaccess_files:\n",
    "            temp.append(lc)\n",
    "    fits_files = temp\n",
    "    \n",
    "    if band == 'all':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "    \n",
    "        return light_curves_1, light_curves_2, light_curves_3\n",
    "    elif band == 'low':\n",
    "        light_curves_1 = [df for df in (load_light_curve(file, band = 0) for file in fits_files) if df is not None]\n",
    "        return light_curves_1\n",
    "    elif band == 'med':\n",
    "        light_curves_2 = [df for df in (load_light_curve(file, band = 1) for file in fits_files) if df is not None]\n",
    "        return light_curves_2\n",
    "    elif band == 'high':\n",
    "        light_curves_3 = [df for df in (load_light_curve(file, band = 2) for file in fits_files) if df is not None]\n",
    "        return light_curves_3\n",
    "    else:\n",
    "        raise KeyError(\"Input for Band is not valid\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff67c5e-f491-4371-b4b8-36015be2dac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m fits_files \u001b[38;5;241m=\u001b[39m load_all_fits_files()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(fits_files))\n\u001b[0;32m----> 4\u001b[0m light_curves_sample \u001b[38;5;241m=\u001b[39m load_n_light_curves(\u001b[38;5;241m400\u001b[39m, fits_files, band \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished loading lcs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(light_curves_sample))\n",
      "Cell \u001b[0;32mIn[9], line 83\u001b[0m, in \u001b[0;36mload_n_light_curves\u001b[0;34m(n, fits_files, band)\u001b[0m\n\u001b[1;32m     80\u001b[0m inaccess_files \u001b[38;5;241m=\u001b[39m read_inaccessible_lightcurves()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Randomly select n files\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m fits_files \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(fits_files, n)\n\u001b[1;32m     85\u001b[0m temp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lc \u001b[38;5;129;01min\u001b[39;00m  fits_files:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.11/random.py:456\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    454\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    457\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    458\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "fits_files = load_all_fits_files()\n",
    "print(len(fits_files))\n",
    "\n",
    "light_curves_sample = load_n_light_curves(400, fits_files, band = \"med\")\n",
    "print('finished loading lcs')\n",
    "print(len(light_curves_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c096350-146d-4139-b371-1e029b29b90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Partition into train set and test set\n",
    "def partition_data(light_curves, test_size=0.2, val_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Partition a list of light curves into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        light_curves (list): List of light curve DataFrames.\n",
    "        test_size (float): Proportion of data to use for the test set.\n",
    "        val_size (float): Proportion of train data to use for the validation set.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_set (list): List of light curves for training.\n",
    "        val_set (list): List of light curves for validation (if val_size > 0).\n",
    "        test_set (list): List of light curves for testing.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    # Split into train+val and test sets\n",
    "    train_val_set, test_set = train_test_split(light_curves, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    if val_size > 0:\n",
    "        # Split train_val into train and validation sets\n",
    "        train_size = 1 - val_size\n",
    "        train_set, val_set = train_test_split(train_val_set, test_size=val_size, random_state=random_seed)\n",
    "        return train_set, val_set, test_set\n",
    "    else:\n",
    "        # If no validation set is needed, return only train and test sets\n",
    "        return train_val_set, test_set\n",
    "train_dataset, val_dataset, test_dataset = partition_data(light_curves_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58949f-a70e-4c1f-a6bf-5098b13b942b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to pad sequences and return lengths.\n",
    "\n",
    "    Parameters:\n",
    "        batch: List of DataFrames representing light curves.\n",
    "\n",
    "    Returns:\n",
    "        x: Padded tensor of shape (batch_size, seq_length, 1).\n",
    "        lengths: Tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    rate = [\n",
    "        torch.tensor(lc['RATE'].values.astype('<f4'), dtype=torch.float32) for lc in batch\n",
    "    ]\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    # Pad sequences\n",
    "    x = pad_sequence(sequences, batch_first=True).unsqueeze(-1)  # Add feature dim\n",
    "\n",
    "    return x, lengths\n",
    "    \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce4793-d1c3-408d-8542-0304f4052cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = RNN_VAE(input_size=1, hidden_size=400, latent_size=50).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Number of epochs\n",
    "nepochs = 10\n",
    "\n",
    "# Initialize lists to store training and validation losses\n",
    "validation_losses = []\n",
    "training_losses = []\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are PyTorch DataLoader objects\n",
    "# The data loaders should yield batches with:\n",
    "# - x: tensor of shape (batch_size, seq_length, 1)\n",
    "# - lengths: tensor of sequence lengths\n",
    "print(\"Beginning training...\")\n",
    "for epoch in range(nepochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x, lengths = batch  # Extract input tensor and lengths\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Forward pass through the VAE\n",
    "        x_hat, mu, logvar = model(x, lengths)\n",
    "\n",
    "        # Compute the ELBO (Evidence Lower Bound) loss\n",
    "        loss = ELBO(x_hat, x, mu, logvar)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        print(1)\n",
    "\n",
    "    # Validation phase (no gradients)\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        valid_loss = 0\n",
    "        for batch in val_loader:\n",
    "            x, lengths = batch\n",
    "            x = x.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            x_hat, mu, logvar = model(x, lengths)\n",
    "            valid_loss += ELBO(x_hat, x, mu, logvar).item()\n",
    "\n",
    "    # Normalize losses by dataset size and append\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "    training_losses.append(train_loss)\n",
    "    validation_losses.append(valid_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} | Train Loss: {train_loss:.2f} | Valid Loss: {valid_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000dc44-a06e-4f6f-b59a-d2e6009ec6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save/load the model weights if you don't want to worry about training\n",
    "torch.save(model.state_dict(), './RNN_VAE_cpu10Epochs.h5')\n",
    "# model.load_state_dict(torch.load('./RNN_VAE_10Epochs.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ef1e2-e003-4b84-82a1-b46ccaf457c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4fc16-50e6-40e0-8542-307ce41c9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    # Grab a single test example (here we assume batch_size=1 in test_loader)\n",
    "    for batch in test_loader:\n",
    "        x_test, lengths_test = batch\n",
    "        x_test = x_test.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat, _, _ = model(x_test, lengths_test)\n",
    "        \n",
    "        # Convert to CPU numpy for plotting\n",
    "        original_curve = x_test.squeeze(-1).cpu().numpy()[0]  # shape: (seq_len,)\n",
    "        reconstructed_curve = x_hat.squeeze(-1).cpu().numpy()[0]  # shape: (seq_len,)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(original_curve, label='Original', alpha=0.7)\n",
    "        plt.plot(reconstructed_curve, label='Reconstructed', alpha=0.7)\n",
    "        plt.title(\"Test Light Curve vs. Reconstruction\")\n",
    "        plt.xlabel(\"Time Index\")\n",
    "        plt.ylabel(\"Rate\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218afe4a-71da-4c53-aecd-e54c3a590aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()  # switch to evaluation mode\n",
    "test_loss = 0.0\n",
    "test_reconstruction_loss = 0.0  # if you'd like a separate reconstruction metric\n",
    "num_samples = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, lengths_test = batch\n",
    "        x_test = x_test.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        x_hat, mu, logvar = model(x_test, lengths_test)\n",
    "\n",
    "        # 1. ELBO Loss\n",
    "        loss = ELBO(x_hat, x_test, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # 2. Additional reconstruction metric (e.g., MSE)\n",
    "        #    We'll assume x_hat and x_test have shape (batch_size, seq_length, features=1)\n",
    "        recon_error = torch.nn.functional.mse_loss(x_hat, x_test, reduction='sum')\n",
    "        test_reconstruction_loss += recon_error.item()\n",
    "\n",
    "        # Keep track of how many total points you had in the batch\n",
    "        num_samples += x_test.size(0) * x_test.size(1)\n",
    "\n",
    "# Divide by the total number of samples to get average losses\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_reconstruction_loss /= num_samples\n",
    "\n",
    "print(f\"Average ELBO on test set: {test_loss:.4f}\")\n",
    "print(f\"Average MSE Reconstruction Error on test set: {test_reconstruction_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1d0e7-935b-4e21-8848-4c1013f19b53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_latent_space(model, data_loader):\n",
    "    \"\"\"\n",
    "    Encodes all data in data_loader using the model's encoder\n",
    "    and returns a numpy array of latent representations.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x, lengths = batch\n",
    "            x = x.to(model.device)\n",
    "\n",
    "            # Forward pass: model returns x_hat, mu, logvar\n",
    "            x_hat, mu, logvar = model(x, lengths)\n",
    "            \n",
    "            # Option 1: Use mu as the latent representation\n",
    "            z = mu\n",
    "            \n",
    "            # Option 2: Reparameterize if you prefer sampling\n",
    "            # z = model.reparameterize(mu, logvar)\n",
    "            \n",
    "            # Convert to CPU and NumPy\n",
    "            z_np = z.cpu().numpy()\n",
    "            \n",
    "            # z_np has shape (batch_size, latent_size)\n",
    "            latent_vectors.append(z_np)\n",
    "    \n",
    "    # Concatenate all batches into one array of shape (num_samples, latent_dim)\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "    return latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed0686-a3d8-4731-ab92-647410fa6284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# 1) Extract Latent\n",
    "####################\n",
    "latent_test = extract_latent_space(model, test_loader)  # shape: (num_test_samples, latent_dim)\n",
    "\n",
    "####################\n",
    "# 2) Fit IsolationForest\n",
    "####################\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    contamination='auto',\n",
    "    random_state=42\n",
    ")\n",
    "iso_forest.fit(latent_test)\n",
    "\n",
    "####################\n",
    "# 3) Predict or Score\n",
    "####################\n",
    "anomaly_labels = iso_forest.predict(latent_test)       # +1 or -1\n",
    "anomaly_scores = iso_forest.decision_function(latent_test)\n",
    "\n",
    "# Example: Count how many outliers were found\n",
    "n_outliers = sum(anomaly_labels == -1)\n",
    "print(f\"Found {n_outliers} outliers in test set.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254d6ae-b3be-4527-b218-dfee019b0027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#   latent_test: array of shape (num_samples, latent_dim)\n",
    "#   anomaly_labels: array of shape (num_samples,) with +1 (inlier) or -1 (outlier)\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "latent_2d = pca.fit_transform(latent_test)  # shape: (num_samples, 2)\n",
    "\n",
    "# OR TSNE\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# latent_2d_tsne = tsne.fit_transform(latent_test)\n",
    "\n",
    "# Separate inliers/outliers for plotting\n",
    "inlier_indices = np.where(anomaly_labels == 1)[0]\n",
    "outlier_indices = np.where(anomaly_labels == -1)[0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(latent_2d[inlier_indices, 0], latent_2d[inlier_indices, 1],\n",
    "            color='blue', alpha=0.5, label='Inliers')\n",
    "plt.scatter(latent_2d[outlier_indices, 0], latent_2d[outlier_indices, 1],\n",
    "            color='red', alpha=0.8, label='Outliers')\n",
    "plt.title(\"PCA of Latent Space with IsolationForest Outliers\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c9cd9-53f1-4d8b-9e77-4d69e33717b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assume:\n",
    "#   anomaly_labels: +1 (inlier), -1 (outlier)\n",
    "#   test_dataset: a list of DataFrames, each with columns TIME, RATE\n",
    "#   outlier_indices: indices where anomaly_labels == -1\n",
    "\n",
    "outlier_indices = np.where(anomaly_labels == -1)[0]\n",
    "print(f\"Number of outliers: {len(outlier_indices)}\")\n",
    "\n",
    "num_to_plot = min(5, len(outlier_indices))\n",
    "random_outlier_indices = np.random.choice(outlier_indices, size=num_to_plot, replace=False)\n",
    "\n",
    "for idx in random_outlier_indices:\n",
    "    light_curve_df = test_dataset[idx]  # Extract the corresponding DataFrame\n",
    "    \n",
    "    # If your DataFrame has columns 'TIME' and 'RATE'\n",
    "    time_vals = light_curve_df['TIME'].values\n",
    "    rate_vals = light_curve_df['RATE'].values\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    # Use scatter instead of plot\n",
    "    plt.scatter(time_vals, rate_vals, color='r', alpha=0.7, marker='o')\n",
    "    plt.title(f\"Outlier Light Curve (Index {idx})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5c6fc-f2a9-43e5-9d7b-5d2883d3f9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now ordered in order of outlierness (most to least)\n",
    "\n",
    "# 2) Order the samples from most anomalous (lowest score) to least anomalous\n",
    "sorted_indices = np.argsort(anomaly_scores)  # ascending order\n",
    "\n",
    "# 3) Now, sorted_indices[0] is the \"most outlier\" sample\n",
    "# most_outlier_index = sorted_indices[0]\n",
    "# least_outlier_index = sorted_indices[-1]\n",
    "\n",
    "# print(f\"Most outlier sample index: {most_outlier_index} with score {anomaly_scores[most_outlier_index]:.4f}\")\n",
    "# print(f\"Least outlier sample index: {least_outlier_index} with score {anomaly_scores[least_outlier_index]:.4f}\")\n",
    "\n",
    "# 4) If you want the top 10 outliers, for instance:\n",
    "top_10_outliers = sorted_indices[:10]\n",
    "print(\"Top 10 Outliers by Index:\", top_10_outliers)\n",
    "\n",
    "actual_outliers_in_top10 = [idx for idx in top_10_outliers if anomaly_labels[idx] == -1]\n",
    "\n",
    "for idx in actual_outliers_in_top10:\n",
    "    # Suppose test_dataset[idx] returns the original time-series (DataFrame, array, etc.)\n",
    "    light_curve_df = test_dataset[idx]  # adapt if needed\n",
    "    time_vals = light_curve_df['TIME'].values\n",
    "    rate_vals = light_curve_df['RATE'].values\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.scatter(time_vals, rate_vals, color='r', alpha=0.7)\n",
    "    plt.title(f\"Outlier Rank: {np.where(top_10_outliers == idx)[0][0] + 1} (Index {idx}), Score={anomaly_scores[idx]:.4f}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Rate\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82269c2c-9be1-4efc-a1bb-dee1444017c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
